{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eadd4ae",
   "metadata": {},
   "source": [
    "# Tutorial \n",
    "\n",
    "**From data to Knowledge Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4fa87e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biokb_ipni version: 0.1.8 Date: 2026-02-03\n"
     ]
    }
   ],
   "source": [
    "# This cell is not import for the understanding of the tutorial\n",
    "# and only prints the version of the biokb_ipni package and the time of execution.\n",
    "import os\n",
    "import biokb_ipni\n",
    "from datetime import datetime\n",
    "\n",
    "# delete existing environment variables\n",
    "if \"CONNECTION_STR\" in os.environ:\n",
    "    del os.environ[\"CONNECTION_STR\"]\n",
    "\n",
    "print(\n",
    "    \"biokb_ipni version:\",\n",
    "    biokb_ipni.__version__,\n",
    "    \"Date:\",\n",
    "    datetime.now().strftime(\"%Y-%m-%d\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be94930e",
   "metadata": {},
   "source": [
    "**Tip**: This notebook is available at GitHub [here](https://github.com/biokb/biokb_ipni/blob/main/docs/notebooks/tutorial.ipynb). It's recommended to set up a virtual environment (see description below), install the biokb_ipni package and run the [Jupyter](https://jupyter.org/) notebook in the virtual environment cell by cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae964bad",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "The [biokb_ipni](https://pypi.org/project/biokb-ipni/) library is part of the [biokb family](https://pypi.org/search/?q=biokb), which aims to integrate ontologies, terminologies, and knowledge from multiple domains—such as biochemistry, pharmacology, taxonomy, and ethnobotany—into a unified Knowledge Graph. Each library - like `biobk_ipni` on plant names - is focusing on a different aspect, but all are using the same workflow to import the data into the database and knowledge graph. Since the biokb family uses [SQLAlchemy](https://www.sqlalchemy.org/) as its database layer, many different relational database systems ([SQLite](https://sqlite.org/), [MySLQ](https://www.mysql.com/), [MariaDB](https://mariadb.org/), [PostgreSQL](https://www.postgresql.org/), ...) can be used. As backend for the knowledge graph `biokb_ipni` is using Neo4j, but any triple store SPARQL server like [Fuseki](https://jena.apache.org/documentation/fuseki2/) can load the Resource Description Framework (RDF) file which can be created with the library. An essential criterion for the high connectivity of the various knowledge graphs (like IPNI) is the library-wide use of the same [Uniform Resource Identifier](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier)s. Each major version of the biokb family libraries will ensure that the same URIs will be used across all libraries.\n",
    "\n",
    "This tutorial describes the use of biokb_ipni to generate a Knowledge Graph from primary data provided by the [International Plant Names Index (IPNI)](https://www.ipni.org/). It also outlines the individual steps required to create either a relational database or RDF Turtle files.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The workflow is simple:\n",
    "\n",
    "[**data**]-*import_data()*->[**relational_database**]-*create_ttls()*->[**rdf_files**]-*import_ttls()*->[**knowledge_graph**] \n",
    "\n",
    "Main functions are:\n",
    "1. `import_data()`: **Import data** in database\n",
    "2. `create_ttls()`: **Create RDF files** from database\n",
    "3. `import_ttls()`: **Import RDF files into a Knowledge Graph**\n",
    "\n",
    "You have choices how to use the library depending:\n",
    "1. Command line interface (CLI)\n",
    "2. Python API\n",
    "3. Podman/Docker containers\n",
    "\n",
    "If you want to use all features of the library it is recommended to use the Podman/Docker containers, since all dependencies are already installed and configured. If you only want to use parts of the library (like only the API or only the RDF generator) you can also install the library in a [virtual environment](https://docs.python.org/3/tutorial/venv.html) and use it via CLI or Python API.\n",
    "\n",
    "## Installation\n",
    "\n",
    "In general it is recommended to install python libraries in a [virtual environment](https://docs.python.org/3/tutorial/venv.html) to avoid conflicts between libraries. To create a virtual environment you first need to create a new directory, navigate to it and activate it. If your operating system is Linux or MacOS run:\n",
    "\n",
    "```bash\n",
    "mkdir biokb_ipni_test\n",
    "cd biokb_ipni_test\n",
    "source .venv/bin/activate\n",
    "```\n",
    "If you are using Windows please check [this tutorial](https://docs.python.org/3/tutorial/venv.html#windows-virtual-environments).\n",
    "\n",
    "Once the virtual environment is activated you can install the biokb_ipni library with pip:\n",
    "\n",
    "```bash\n",
    "pip install biokb_ipni\n",
    "```\n",
    "\n",
    "**Tip**: [uv](https://docs.astral.sh/uv/) is a great and very fast alternative tool to create and manage virtual environments.\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "Depending on your needs, follow the links:\n",
    "\n",
    "1. [Relational database only](#relational-database-only)\n",
    "2. [RDF turtle files only](#rdf-turtle-file-only)\n",
    "3. [Knowledge Graph](#knowledge-graph-in-neo4j)\n",
    "\n",
    "In the different chapters you will find step-by-step instructions how to achieve your goal (CLI, RESTful API, Python API, or Podman/Docker containers).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f507c8",
   "metadata": {},
   "source": [
    "### Relational database only\n",
    "\n",
    "\n",
    "**Tip**: SQLite database can be opened with [DB Browser for SQLite](https://sqlitebrowser.org/), which provides a user-friendly interface to explore the database structure and content. Or with the [SQLite Viewer extension](https://marketplace.visualstudio.com/items?itemName=alexcvzz.vscode-sqlite) for [VS Code](https://code.visualstudio.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d38e4",
   "metadata": {},
   "source": [
    "#### Python\n",
    "\n",
    "by default creates (or updates) a SQLite database `biokb.db` in subfolder of your home directory `~/.biokb/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab1999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biokb_ipni import import_data\n",
    "\n",
    "import_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802e4427",
   "metadata": {},
   "source": [
    "If you want to change the logging level you can do it as follows:\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from biokb_ipni import import_data\n",
    "logging.getLogger('biokb_ipni').setLevel(logging.WARNING)\n",
    "import_data()\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "2026-01-28 14:18:54,188 - biokb_ipni.db.manager - INFO - Using database engine: Engine(sqlite:////home/ceb/.biokb/biokb.db)\n",
    "2026-01-28 14:18:54,231 - biokb_ipni.db.manager - INFO - Database recreated.\n",
    "2026-01-28 14:18:54,231 - biokb_ipni.db.manager - INFO - Loading NCBI Taxonomy data for mapping families and names\n",
    "2026-01-28 14:19:06,912 - biokb_ipni.db.manager - INFO - Importing references\n",
    "2026-01-28 14:19:17,348 - biokb_ipni.db.manager - INFO - Importing families\n",
    "2026-01-28 14:19:19,166 - biokb_ipni.db.manager - INFO - Importing names\n",
    "2026-01-28 14:20:09,802 - biokb_ipni.db.manager - INFO - Importing type materials\n",
    "2026-01-28 14:20:15,326 - biokb_ipni.db.manager - INFO - Importing name relations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e00c33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import all data in database.\n",
      "\n",
      "Args:\n",
      "    engine (Optional[Engine]): SQLAlchemy engine. Defaults to None.\n",
      "    force_download (bool, optional): If True, will force download the data, even if\n",
      "        files already exist. If False, it will skip the downloading part if files\n",
      "        already exist locally. Defaults to False.\n",
      "    delete_files (bool, optional): If True, downloaded files are deleted after import.\n",
      "        Defaults to False.\n",
      "\n",
      "Returns:\n",
      "    Dict[str, int]: table=key and number of inserted=value\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(import_data.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267b0c50",
   "metadata": {},
   "source": [
    "#### Command line interface\n",
    "\n",
    "```bash\n",
    "biokb_ipni -v import-data\n",
    "```\n",
    "\n",
    "`-v` provides you with information about the processes currently running. If you do not wish to see this information, you can simply omit `-v`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94cf41",
   "metadata": {},
   "source": [
    "##### Options with the CLI\n",
    "\n",
    "```bash\n",
    "  -f, --force-download          Force re-download of the source file [default: False]\n",
    "  -d, --delete-files            Delete downloaded source files after import [default: False]\n",
    "  -c, --connection-string TEXT  SQLAlchemy engine URL [default:sqlite:////~/.biokb/biokb.db]\n",
    "  --help                        Show this message and exit.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981a5cb",
   "metadata": {},
   "source": [
    "Here an example to create a database with a custom name, force re-download of the source file, and delete downloaded source files after import:\n",
    "\n",
    "```bash\n",
    "biokb_ipni -v import-data -f -d -c sqlite:///my_own_name.db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc6fc0",
   "metadata": {},
   "source": [
    "#### How to write a connection string\n",
    "\n",
    "In the last (and also in the next) section we used a connection string to define the type and name of the database. Here are some examples how to write connection strings for different database management systems:\n",
    "\n",
    "- SQLite: `sqlite:///ipni.db` (creates a file named `ipni.db` in the current directory)\n",
    "- MySQL: `mysql+pymysql://username:password@localhost/ipni_db`\n",
    "- PostgreSQL: `postgresql+psycopg2://username:password@localhost/ipni_db`\n",
    "\n",
    "If you are using a different host or port, please adjust the connection string accordingly. For more details on connection strings, please refer to the [SQLAlchemy documentation](https://docs.sqlalchemy.org/en/20/core/engines.html#database-urls). SQLite and MySQL are supported out of the box. For other databases you need to install the specific libraries (like `psycopg2` package) in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93352004",
   "metadata": {},
   "source": [
    "#### Python with other database management systems\n",
    "\n",
    "If we want to use a different database like the default (SQLite -> ~/.biokb/biokb.db) in python we have to create an engine with SQLAlchemy first and then pass it to the `import_data` function. Here an example with MySQL (assuming you have already created a database named `ipni_db` in your MySQL server, *host:localhost, port:3306, database: ipni, user: username, password: password*):\n",
    "\n",
    "```python\n",
    "from sqlalchemy import create_engine\n",
    "from biokb_ipni import import_data\n",
    "\n",
    "engine = create_engine(\"mysql+pymysql://username:password@localhost:3306/ipni\")\n",
    "import_data(engine)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9990dc2e",
   "metadata": {},
   "source": [
    "## RESTful API only\n",
    "\n",
    "If you only want to use the RESTful API to access the data you can start it as follows:\n",
    "\n",
    "\n",
    "```bash\n",
    "biokb_ipni run-server\n",
    "```\n",
    "\n",
    "If you get an error message like:\n",
    "\n",
    "```\n",
    "biokb_ipni run-server\n",
    "API server running at http://127.0.0.1:8000/docs#/\n",
    "ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8000): address already in use\n",
    "```\n",
    "\n",
    "Use another port with the `--port` option:\n",
    "\n",
    "```bash\n",
    "biokb_ipni run-server --port 8080\n",
    "```\n",
    "\n",
    "Depending on the port (here 8080) open http://127.0.0.1:8080/docs#/ in your browser to access the API documentation.\n",
    "\n",
    "By default username `admin` and password `admin` are used to access the protected methods of the API (with a lock symbol). You can change it with the options `--user` and `--password`:\n",
    "\n",
    "```bash\n",
    "biokb_ipni run-server --user my_user --password my_password\n",
    "```\n",
    "\n",
    "If data already import via `biokb_ipni import-data` the API will use the existing database located at `~/.biokb/biokb.db`. Otherwise open the `/import_data/` endpoint to import the data (use user and password). ttls files will be created (if not exists) and exported via the `/export_data/` endpoint. If you want to import the data into a Neo4j Knowledge Graph use `/import_neo4j/`.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biokb-ipni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
